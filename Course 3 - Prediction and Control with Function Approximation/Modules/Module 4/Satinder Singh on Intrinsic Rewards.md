# Intrinsic Rewards in Reinforcement Learning: Key Concepts and Formulas

## 1. What Are Intrinsic Rewards?

- **Definition:**  
  Intrinsic rewards are internal signals generated by the agent, independent of the environment's external (extrinsic) rewards, to encourage behaviors such as exploration, curiosity, or skill acquisition[6][7][8].
- **Contrast with Extrinsic Rewards:**  
  - *Extrinsic rewards* are provided by the environment and define the main task (e.g., reaching a goal, collecting points)[6][7].
  - *Intrinsic rewards* are designed or learned to improve learning dynamics, such as by motivating the agent to explore novel states or learn useful skills[6][8].

---

## 2. Why Use Intrinsic Rewards?

- **Sparse or Deceptive Extrinsic Rewards:**  
  In many environments, extrinsic rewards are rare or misleading, making it hard for agents to learn effective behaviors[6][8].
- **Improved Exploration:**  
  Intrinsic rewards can drive agents to explore more efficiently, discovering useful states and behaviors that may not be immediately rewarded by the environment[8][12].
- **Skill and Knowledge Acquisition:**  
  By rewarding novelty, prediction error, or information gain, intrinsic rewards help agents acquire general skills and knowledge that transfer across tasks[8][12].

---

## 3. Types of Intrinsic Rewards

- **Novelty-Based:**  
  Reward the agent for visiting novel or less-visited states, often using state visitation counts or prediction error as a proxy for novelty[8][12].
  
  $$
  r_t^{\text{intrinsic}} = f(\text{novelty}(s_t))
  $$
- **Curiosity-Driven:**  
  Reward the agent for transitions that are hard to predict, using the error in a learned dynamics model[12].
  
  $$
  r_t^{\text{intrinsic}} = \| s_{t+1} - \hat{s}_{t+1} \|
  $$
  
  where $\hat{s}_{t+1}$ is the predicted next state.
- **Information Gain:**  
  Reward the agent for actions that maximize information about the environment or its own model[8].
- **Empowerment/Skill Discovery:**  
  Reward the agent for acquiring control or discovering diverse skills, often using mutual information or diversity objectives[8].

---

## 4. The Optimal Reward Framework

- **Formalization:**  
  The *optimal reward framework* distinguishes between extrinsic rewards (task-defined) and intrinsic rewards (agent-defined for learning)[6][8].
- **Objective:**  
  Learn an intrinsic reward function $r_{\text{int}}$ that, when combined with extrinsic rewards, maximizes the agent's long-term extrinsic performance[6][8].
  
  $$
  r^*_{\mathcal{A}} = \arg\max_{r_{\mathcal{A}} \in \mathcal{R}_{\mathcal{A}}} \mathbb{E}_{E \sim P(\varepsilon)} \mathbb{E}_{h \sim \langle A(r_{\mathcal{A}}), E \rangle} \left\{ \mathcal{F}(h) \right\}
  $$
  
  where $\mathcal{F}(h)$ is the extrinsic fitness of history $h$[7][8].

---

## 5. Learning Intrinsic Rewards

- **Meta-Learning Approach:**  
  Use meta-gradients to learn intrinsic reward functions that generalize across tasks and agent lifetimes[8][10].
- **Bi-Level Optimization:**  
  Treat the problem as a bi-level optimization: the agent's policy is updated using intrinsic rewards, and the intrinsic reward function is updated to maximize extrinsic performance[10].
- **Update Example:**  
  For policy parameters $\theta$ and intrinsic reward parameters $\phi$:
  
  $$
  \theta' = \theta + \alpha \nabla_\theta \mathbb{E}[r_{\text{ext}} + r_{\text{int}}(s, a; \phi)]
  $$
  
  $$
  \phi' = \phi + \beta \nabla_\phi \mathbb{E}[r_{\text{ext}}(\theta')]
  $$
  
  where $r_{\text{ext}}$ is the extrinsic reward[8][10].

---

## 6. Practical Examples

- **Count-Based Exploration:**  
  Intrinsic reward is inversely proportional to the square root of state visitation count:
  
  $$
  r_t^{\text{intrinsic}} = \frac{1}{\sqrt{N(s_t)}}
  $$
  
  where $N(s_t)$ is the number of times state $s_t$ has been visited[8][12].
- **Curiosity with Prediction Error:**  
  Use the error of a learned forward model as intrinsic reward:
  
  $$
  r_t^{\text{intrinsic}} = \| f(s_t, a_t) - s_{t+1} \|^2
  $$
  
  where $f$ is the learned model predicting the next state[12].

---

## 7. Advantages and Challenges

### Advantages

- **Improved Exploration:**  
  Agents can discover rewarding behaviors in sparse or deceptive environments[8][12].
- **Transferability:**  
  Learned intrinsic rewards can generalize across agents and environments, capturing “what to do” rather than “how to do”[8][11].

### Challenges

- **Reward Hacking:**  
  Agents may exploit intrinsic rewards in unintended ways, leading to suboptimal or unsafe behaviors[7][9].
- **Balancing Intrinsic and Extrinsic Rewards:**  
  The agent must not focus solely on intrinsic rewards at the expense of the main task[6][8].
- **Design Complexity:**  
  Designing or learning effective intrinsic rewards is nontrivial and may require meta-learning or bi-level optimization[8][10].

---

## 8. Key Takeaways

- Intrinsic rewards are internal signals that drive exploration, curiosity, and skill acquisition in RL agents[6][8].
- They are especially useful in environments with sparse or deceptive extrinsic rewards[8][12].
- Modern approaches use meta-learning and bi-level optimization to learn intrinsic rewards that maximize long-term extrinsic performance[8][10].
- Balancing intrinsic and extrinsic rewards is crucial for effective learning and safe agent behavior[6][7][8].
