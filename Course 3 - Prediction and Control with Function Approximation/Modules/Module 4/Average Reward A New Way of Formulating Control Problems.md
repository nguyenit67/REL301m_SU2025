# Average Reward in Reinforcement Learning

## 1. What is Average Reward?

- **Definition:**  
  Average reward is a formulation in reinforcement learning (RL) that focuses on maximizing the long-term average reward per time step, rather than the discounted sum of rewards[1].
- **Objective:**  
  The agent seeks a policy $\pi$ that maximizes the expected average reward:
  
  $$
  \bar{R}_\pi = \lim_{T \to \infty} \frac{1}{T} \mathbb{E}_\pi \left[ \sum_{t=1}^T R_t \right]
  $$
  
  where $R_t$ is the reward at time $t$ and the expectation is over trajectories generated by policy $\pi$[1].

---

## 2. Why Use Average Reward?

- **Simplicity:**  
  Simplifies analysis and computation of optimal policies, especially in continuing tasks[1].
- **Stationarity:**  
  Suitable for environments with unknown episode lengths or varying dynamics, as it does not require a discount factor[1].
- **Performance Metric:**  
  Provides a clear, directly optimizable performance metric for RL algorithms[1].

---

## 3. Comparison: Discounted vs. Average Reward

| Formulation       | Objective                                      | Formula                                                                                        |
| ----------------- | ---------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| Discounted Reward | Maximize expected sum of discounted rewards    | $G_t = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right]$                     |
| Average Reward    | Maximize expected average reward per time step | $\bar{R}_\pi = \lim_{T \to \infty} \frac{1}{T} \mathbb{E}_\pi \left[ \sum_{t=1}^T R_t \right]$ |

- **Discounted reward** uses a discount factor $\gamma$ to prioritize immediate rewards, while **average reward** treats all time steps equally[1].

---

## 4. Value Functions for Average Reward

- **Differential Value Function:**  
  Measures the expected excess reward over the average, starting from state $s$ under policy $\pi$:
  
  $$
  v_\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty (R_{t+1} - \bar{R}_\pi) \mid S_0 = s \right]
  $$
  
  where $R_{t+1}$ is the reward at time $t+1$ and $\bar{R}_\pi$ is the average reward under policy $\pi$[1].

- **Bellman Equation for Differential Value:**
  
  $$
  v_\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} - \bar{R}_\pi + v_\pi(S_{t+1}) \mid S_t = s \right]
  $$
  
  This equation relates the differential value of a state to the expected immediate reward, the average reward, and the value of the next state[1].

---

## 5. Advantages and Disadvantages

### Advantages

- **No Need for Large $\gamma$:**  
  Avoids the need to set $\gamma$ close to 1, which can cause large, variable sums and slow learning[1].
- **Directly Optimizes Long-Term Performance:**  
  Focuses on maximizing the true long-term rate of reward, not just short-term gains[1].

### Disadvantages

- **No Time Preference:**  
  Does not explicitly prioritize immediate rewards, which may be important in some tasks[1].
- **Convergence Challenges:**  
  Finding optimal policies can be harder, especially in non-stationary or complex environments[1].

---

## 6. Differential Sarsa (Average Reward Sarsa)

- **Purpose:**  
  Extends Sarsa to the average reward setting by estimating both the average reward $\bar{R}_\pi$ and the differential value function[1].
- **Update Rule:**  
  For each transition $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$:
  
  $$
  \delta_t = R_{t+1} - \bar{R}_t + Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)
  $$
  
  $$
  Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \delta_t
  $$
  
  $$
  \bar{R}_{t+1} \leftarrow \bar{R}_t + \beta \delta_t
  $$
  
  where $\alpha$ and $\beta$ are step-size parameters[1].

---

## 7. Key Takeaways

- **Average reward RL** is well-suited for continuing tasks and environments with unknown or infinite horizons[1].
- **Differential value functions** and their Bellman equations are central to average reward methods[1].
- **Differential Sarsa** is a practical algorithm for learning in the average reward setting[1].
