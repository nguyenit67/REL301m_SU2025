# Course 1 - Fundamentals of Reinforcement Learning

## Module 2

New terms:

short/long-term reward

policies

planning methods

dynamic programming

reward 

time steps

#### Video: Sequential Decision Making with Evaluative Feedback

##### Action-Value function

- GiÃ¡ trá»‹ cá»§a hÃ nh Ä‘á»™ng ($q_*$) lÃ  giÃ¡ trá»‹ ká»³ vá»ng cá»§a táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ kháº£ thi khi thá»±c hiá»‡n hÃ nh Ä‘á»™ng *a*
  
  $$
  \begin{align*}
q_*(a) &\doteq \mathbb{E}[R_t \mid A_t = a] \quad \forall a \in \{1, \ldots, k\} \\
&= \sum_{r} p(r \mid a)\, r
\end{align*}
  $$
  
  GiÃ¡ trá»‹ cá»§a hÃ nh Ä‘á»™ng $q$ lÃ  sá»‘ chÆ°a biáº¿t -> cáº§n Ä‘Æ°á»£c Æ°á»›c tÃ­nh!
  $q_*$: giÃ¡ trá»‹ ká»³ vá»ng thá»±c sá»±
  $q$: giÃ¡ trá»‹ ká»³ vá»ng Æ°á»›c tÃ­nh

- Má»¥c tiÃªu lÃ  chá»n hÃ nh Ä‘á»™ng *a* Ä‘á»ƒ tá»‘i Ä‘a hÃ³a pháº§n thÆ°á»Ÿng/giÃ¡ trá»‹ ká»³ vá»ng cá»§a hÃ nh Ä‘á»™ng
  
  $$
  \underset{a}{\arg\max}\; q_*(a)
  $$

![69ddb2b8-9454-4cf4-9c3f-c35c67177e87](file:///D:/Disk/Computer/Pictures/Typedown/69ddb2b8-9454-4cf4-9c3f-c35c67177e87.png) 

![98116610-cbc8-4a9f-81f0-fbd2d5c5ef17](file:///D:/Disk/Computer/Pictures/Typedown/98116610-cbc8-4a9f-81f0-fbd2d5c5ef17.png)

How is the bandit problem similar to or different from the supervised learning problem?

**Vietnamese**

Giá»‘ng: cáº£ 2 Ä‘á»u cÃ³ má»¥c tiÃªu Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘i Æ°u Ä‘Æ°á»£c Ä‘o lÆ°á»ng bá»Ÿi 1 hÃ m sá»‘ (supervised: loss function, bandit problem: q*/reward function), supervised Ä‘Æ°á»£c train trÃªn 1 táº­p data há»¯u háº¡n vÃ  cá»‘ Ä‘á»‹nh, bandit problem thÃ¬ cÃ³ sá»‘ lÆ°á»£ng action lÃ  1 táº­p há»¯u háº¡n cÃ¡c action (K).

KhÃ¡c: supervised learning tá»‘i Ä‘a hÃ³a hÃ m máº¥t mÃ¡t trÃªn 1 táº­p data cá»‘ Ä‘á»‹nh, ko thay Ä‘á»•i, label lÃ  cá»‘ Ä‘á»‹nh; bandit problem thÃ¬ cÃ³ label lÃ  giÃ¡ trá»‹ ká»³ vá»ng trÃªn 1 phÃ¢n phá»‘i xÃ¡c suáº¥t.

**English**

##### **Similarities**

1. ###### **Optimization Objective**
   
   * Both aim to optimize a measurable function:
     
     * _Supervised Learning_: Minimizes aÂ **loss function**Â (e.g., cross-entropy, MSE).
     
     * _Bandit Problems_: Maximizes aÂ **reward function**Â (e.g., Q*-value, expected reward).

2. ###### **Finite Action Space**
   
   * Supervised learning uses a fixed, finite dataset.
   
   * Bandit problems assume a finite set ofÂ **K actions**Â (e.g., choosing between K ad variants).

##### **Key Differences**

| **Aspect**               | **Supervised Learning**                        | **Bandit Problems**                                                   |
| ------------------------ | ---------------------------------------------- | --------------------------------------------------------------------- |
| **Data Dynamics**        | Static dataset with fixed labels               | Dynamic, stochastic rewards from a distribution                       |
| **Feedback Type**        | Full feedback (labels for all inputs)          | Partial feedback (reward only for chosen action)                      |
| **Exploration Strategy** | No exploration needed (deterministic training) | RequiresÂ **exploration-exploitation trade-off**Â (e.g., Îµ-greedy, UCB) |
| **Objective**            | Generalize to unseen data                      | Maximize cumulative reward over interactions                          |

#### Video: Learning Action Values

##### Sample-Average Method

$$
\begin{align*}
Q_t(a) &\doteq \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} \\\\
      &= \frac{\sum_{i=1}^{t-1} R_i}{t - 1}
\end{align*}
$$

##### Greedy action

Method of choosing action: choosing the **greedy action** a.k.a the action currently has the largest estimated value

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/282fccc4-37e6-4625-a43f-cd80ab15de68.png" alt="282fccc4-37e6-4625-a43f-cd80ab15de68" data-align="center">

#### Video: Estimating Action Values Incrementally

##### <u><mark>Incremental update rule</mark></u>

![701f6f29-67dd-4d85-88e1-8f29879f9ba7](file:///D:/Disk/Computer/Pictures/Typedown/701f6f29-67dd-4d85-88e1-8f29879f9ba7.png)

$$
\textit{NewEstimate} \gets \textit{OldEstimate} + \textit{StepSize} \left[ \textit{Target} - \textit{OldEstimate} \right] 
\\ \\
Q_{n+1} = Q_n + \alpha_n \left( R_n - Q_n \right)
\\ \\
\alpha_n \to [0,1]

$$

Sample average method

$$
\alpha_n = \frac{1}{n} \\[1.5em]
\alpha \space \text{(step size),} \space \text{lÃ  1 háº±ng sá»‘}
$$

##### **Non-stationary bandit problem (rewad ko cá»‘ Ä‘á»‹nh vÃ  cÃ³ thá»ƒ thay Ä‘á»•i  theo thá»i gian)**

Trong cÃ¡c bÃ i toÃ¡n non-stationary (cÃ³ tÃ­nh thay Ä‘á»•i theo thá»i gian, há»c trá»±c tuyáº¿n a.k.a online learning), stepsize lÃ  1 háº±ng sá»‘ cá»‘ Ä‘á»‹nh sáº½ giÃºp estimate vÃ  Ä‘iá»u chá»‰nh $q$ tá»‘t vÃ  nhanh hÆ¡n lÃ  1 step size giáº£m dáº§n theo thá»i gian (decaying: $\frac{1}{n}$)

![a10bd3be-0c30-49bc-9b6e-ee49677ad36f](file:///D:/Disk/Computer/Pictures/Typedown/a10bd3be-0c30-49bc-9b6e-ee49677ad36f.png)

the most recent rewards affect the estimate more than older rewards (cÃ¡c reward má»›i nháº¥t áº£nh hÆ°á»Ÿng Ä‘áº¿n giÃ¡ trá»‹ Æ°á»›c lÆ°á»£ng hÆ¡n cÃ¡c reward á»Ÿ cÃ¡c bÆ°á»›c xa hÆ¡n)



##### **Decaying past rewards**

$$
\begin{align*}
Q_{n+1} &= Q_n + \alpha_n \left( R_n - Q_n \right) \\
        &= \alpha_n R_n + Q_n - \alpha_n Q_n \\
        &= \alpha_n R_n + (1 - \alpha_n) Q_n \\
        &= \alpha_n R_n + (1 - \alpha_n)\left[\alpha_n R_{n-1} + (1 - \alpha_n) Q_{n-1}\right] \\
        &= \alpha_n R_n + (1 - \alpha_n)\alpha_n R_{n-1} + (1 - \alpha_n)^2 Q_{n-1} \\
        &= \alpha_n R_n + (1 - \alpha_n)\alpha_n R_{n-1} + (1 - \alpha_n)^2\alpha_n R_{n-2} + \dots \\
        &\quad + (1 - \alpha_n)^{n-1}\alpha_n R_1 + (1 - \alpha_n)^n Q_1 \\[\quad]
        &= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i

    &\\\\
    &Q_1 \to \textbf{initial action-value}

\end{align*}
$$

ÄÃ³ng gÃ³p cá»§a Q_1 vá»›i giÃ¡ trá»‹ Æ°á»›c tÃ­nh táº¡i bÆ°á»›c $n+1$ giáº£m dáº§n theo cáº¥p lÅ©y thá»«a theo thá»i gian, cÃ¡c giÃ¡ trá»‹ reward á»Ÿ cÃ¡c bÆ°á»›c cÅ© Ä‘Ã³ng gÃ³p theo cáº¥p lÅ©y thá»«a Ã­t hÆ¡n. Sá»± áº£nh hÆ°á»Ÿng cá»§a giÃ¡ trá»‹ khá»Ÿi táº¡o ($Q_1$) tiáº¿n gáº§n vá» $0$ khi cÃ ng cÃ³ thÃªm nhiá»u data, cÃ¡c giÃ¡ trá»‹ má»›i nháº¥t quyáº¿t Ä‘á»‹nh giÃ¡ trá»‹ Æ°á»›c tÃ­nh hiá»‡n táº¡i ($Q_{n+1}$) 

### Exploration vs. Exploitation Tradeoff

#### Video: What is the trade-off?

##### Exploration and Exploitation

- **Exploration** - **improve** knowledge for **long-term** benefit

- **Exploitation** - **exploit** knowledge for **short-term** benefit (being greedy w.r.t estimated values, may not actually get the most reward)

- Round Robin fashion: tuáº§n tá»± theo chu ká»³

CÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ cÃ¢n báº±ng giá»¯a Exploration vÃ  Exploitation

##### Epsilon-Greedy Action Selection

$$
A_t \leftarrow
\begin{cases}
    \displaystyle \arg\max_{a} Q_t(a) & \text{with probability } 1 - \epsilon \\\\
    a \sim \mathrm{Uniform}(\{a_1, \ldots, a_k\}) & \text{with probability } \epsilon
\end{cases}
$$

Lá»±a chá»n khÃ¡m phÃ¡ (explore) ngáº«u nhiÃªn 1 hÃ nh Ä‘á»™ng vá»›i xÃ¡c suáº¥t $\epsilon$ tá»« xÃ¡c suáº¥t phÃ¢n phá»‘i Ä‘á»u, vÃ  lá»±a chá»n greedy hÃ nh Ä‘á»™ng cÃ³ Æ°á»›c tÃ­nh pháº§n thÆ°á»Ÿng $Q_t$ lá»›n nháº¥t trong sá»‘ cÃ¡c hÃ nh Ä‘á»™ng (exploit)

#### Video:Â Optimistic Initial Values

![8c330cdf-be2e-4b0f-bd53-34a7666ded27](file:///D:/Disk/Computer/Pictures/Typedown/8c330cdf-be2e-4b0f-bd53-34a7666ded27.png)

Khá»Ÿi táº¡o giÃ¡ trá»‹ ká»³ vá»ng Æ°á»›c lÆ°á»£ng khá»Ÿi Ä‘áº§u cao vÃ  thá»±c hiá»‡n chiáº¿n thuáº­t lá»±a chá»n tham lam (greedy selection) giÃºp agent cÃ³ thá»ƒ explore cÃ¡c lá»±a chá»n khÃ¡c nhau á»Ÿ cÃ¡c timestep Ä‘áº§u tiÃªn vÃ  update dáº§n vá» giÃ¡ trá»‹ hÃ nh Ä‘á»™ng thá»±c táº¿

![9b941a14-20b5-436c-b00f-75df53c628d4](file:///D:/Disk/Computer/Pictures/Typedown/9b941a14-20b5-436c-b00f-75df53c628d4.png)

**Giá»›i háº¡n**:

- Chá»‰ explore á»Ÿ cÃ¡c bÆ°á»›c Ä‘áº§u tiÃªn, sau khi Ä‘áº¿n bÆ°á»›c nÃ o Ä‘Ã³ sáº½ dá»«ng explore

- Ko phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n cÃ³ reward thay Ä‘á»•i theo thá»i gian (non-stationary problems)

- KhÃ´ng thá»ƒ biáº¿t Ä‘Æ°á»£c giÃ¡ trá»‹ khá»Ÿi Ä‘áº§u láº¡c quan nÃªn Ä‘á»ƒ lÃ  bao nhiÃªu (vÃ¬ khÃ´ng biáº¿t giÃ¡ trá»‹ tá»‘i Ä‘a cá»§a reward)

#### Video:Â Upper-Confidence Bound (UCB) Action Selection

<img src="file:///D:/Disk/Computer/Pictures/Typedown/9708c4b5-0bd3-469e-990d-4a21f65f0bef.png" title="" alt="9708c4b5-0bd3-469e-990d-4a21f65f0bef" data-align="center">**Optimism in the Face of Uncertainty**

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/5b9c12ec-9468-4456-9a09-08757586c767.png" alt="5b9c12ec-9468-4456-9a09-08757586c767" data-align="center" style="zoom:67%;">

##### Upper-Confidence Bound (UCB) Action Selection

chá»n action cÃ³ cáº­n trÃªn cá»§a giÃ¡ trá»‹ hÃ nh Ä‘á»™ng lÃ  cao nháº¥t

$$
A_t \doteq \argmax_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \;\right]
$$

$c \sqrt{\frac{\ln t}{N_t(a)}}\;$ is upper-confidence bound (UCB) exploration term

$c$ is user-specified parameter that controls the amount of exploration

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/18a97f24-8a9d-4624-87b0-638931dc1d9f.png" alt="18a97f24-8a9d-4624-87b0-638931dc1d9f" style="zoom:50%;" data-align="center">

#### Video:Â Jonathan Langford: Contextual Bandits for Real World Reinforcement Learning

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/8910efa2-9370-488b-8f87-bb6e540c075d.png" alt="8910efa2-9370-488b-8f87-bb6e540c075d" style="zoom:25%;" data-align="center">

There's is a gap between the simulator and the reality.

##### **Real World Reinforcement Learning**

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/b4fa794d-6953-4b4e-b544-cf66fcd002f9.png" alt="b4fa794d-6953-4b4e-b544-cf66fcd002f9" style="zoom:25%;" data-align="center">

##### **An example: Contextual Bandits**

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/823786ac-433a-4e44-916a-9145bc55d585.png" alt="823786ac-433a-4e44-916a-9145bc55d585" data-align="center" style="zoom:33%;">

Having a set of possible news articles in interest today, suggest news article for new user come to website that they maybe have interest in, get feedback about whether or not they actually read the article. 

Repeatedly:

1. Observe features $\textcolor{red}{x}$ (user geolocation, past profile behaviour, features of available actions,...)

2. Choose action $\textcolor{red}{a \in A}$

3. Observe reward $\textcolor{red}{r}$

**Goal: Maximize reward**

Major caveat: No credit assignment (for selected action, what's the reward if we had chosen a different action?)

#### Video: Week 1 Summary

Exploration vs Exploitation strategies

- Îµ-Greedy Action Selection

- Optimistic Initial Values

- Upper-Confidence Bound (UCB) Action Selection

## Module 3

### Introduction to Markov Decision Processes

#### Video: Markov Decision Processes

* CÃ¡c tÃ¬nh huá»‘ng khÃ¡c nhau sáº½ xuáº¥t hiá»‡n cÃ¡c lá»±a chá»n hÃ nh Ä‘á»™ng khÃ¡c nhau

* HÃ nh Ä‘á»™ng dáº«n Ä‘áº¿n tráº¡ng thÃ¡i cá»§a tháº¿ giá»›i thay Ä‘á»•i action -> state

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/83a0a0d4-1bee-4ab7-ad10-a54f882c84d9.png" alt="83a0a0d4-1bee-4ab7-ad10-a54f882c84d9" data-align="center" style="zoom:50%;">

**Markov Decision Processes (MDPs)** lÃ  cÃ¡c khung toÃ¡n há»c Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a cÃ¡c váº¥n Ä‘á» ra quyáº¿t Ä‘á»‹nh, trong Ä‘Ã³ má»™t tÃ¡c nhÃ¢n tÆ°Æ¡ng tÃ¡c vá»›i má»™t mÃ´i trÆ°á»ng trong má»™t chuá»—i cÃ¡c bÆ°á»›c thá»i gian riÃªng biá»‡t (discrete time steps).

##### Markov Decision Processes terms:

- **Agent**: lÃ  thá»±c thá»ƒ mÃ  chÃºng ta Ä‘ang huáº¥n luyá»‡n Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c quyáº¿t Ä‘á»‹nh chÃ­nh xÃ¡c.

- **Environment**: mÃ´i trÆ°á»ng xung quanh mÃ  agent tÆ°Æ¡ng tÃ¡c.
* **State**: (thÃ´ng tin) tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a agent trong mÃ´i trÆ°á»ng Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh.

* **Action**: lá»±a chá»n mÃ  agent thá»±c hiá»‡n táº¡i má»—i bÆ°á»›c thá»i gian hiá»‡n táº¡i.

* **Policy**: Policy lÃ  quÃ¡ trÃ¬nh suy nghÄ© hoáº·c chiáº¿n lÆ°á»£c Ä‘á»©ng sau viá»‡c lá»±a chá»n má»™t action.

* **P (Transition Probability):**Â `P(s'|s, a)`Â cho biáº¿t xÃ¡c suáº¥t chuyá»ƒn sang tráº¡ng thÃ¡iÂ `s'`Â khi thá»±c hiá»‡n hÃ nh Ä‘á»™ngÂ `a`Â táº¡i tráº¡ng thÃ¡iÂ `s`. Äiá»u nÃ y thá»ƒ hiá»‡n tÃ­nh ngáº«u nhiÃªn cá»§a mÃ´i trÆ°á»ng.

* **R (Reward Function):**Â `R(s, a, s')`Â xÃ¡c Ä‘á»‹nh pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c ngay láº­p tá»©c sau khi chuyá»ƒn tá»« tráº¡ng thÃ¡iÂ `s`Â sang tráº¡ng thÃ¡iÂ `s'`Â do thá»±c hiá»‡n hÃ nh Ä‘á»™ngÂ `a`.

Sometimes:

* **Î³ (Discount Factor):**Â Má»™t giÃ¡ trá»‹ trong khoáº£ng tá»« 0 Ä‘áº¿n 1, quyáº¿t Ä‘á»‹nh má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c pháº§n thÆ°á»Ÿng trong tÆ°Æ¡ng lai.

##### How MDPs Work

á» má»—i bÆ°á»›c thá»i gian:

* Agent quan sÃ¡t tráº¡ng thÃ¡i hiá»‡n táº¡i.

* Chá»n má»™t action dá»±a trÃªn policy (má»™t Ã¡nh xáº¡ tá»« state Ä‘áº¿n action).

* Environment chuyá»ƒn sang má»™t state má»›i dá»±a trÃªn xÃ¡c suáº¥t chuyá»ƒn tiáº¿p (transition probabilities).

* Agent nháº­n Ä‘Æ°á»£c reward tÆ°Æ¡ng á»©ng vá»›i action Ä‘Ã£ thá»±c hiá»‡n.

Má»¥c tiÃªu lÃ  tÃ¬m ra má»™tÂ **chÃ­nh sÃ¡ch tá»‘i Æ°u**Â  Ä‘á»ƒ tá»‘i Ä‘a hÃ³a tá»•ng pháº§n thÆ°á»Ÿng ká»³ vá»ng (expected cumulative reward) theo thá»i gian.

##### The dynamics of an MDP (Markov property)

**Khi agent thá»±c hiá»‡n hÃ nh Ä‘á»™ngÂ `a`Â trong tráº¡ng thÃ¡iÂ `s`**, sáº½ cÃ³ nhiá»u tráº¡ng thÃ¡i káº¿ tiáº¿pÂ $s'$Â vÃ  pháº§n thÆ°á»ŸngÂ $r$Â cÃ³ thá»ƒ xáº£y ra. HÃ m Ä‘á»™ng lá»±c chuyá»ƒn tiáº¿p $P(s', r | s, a)$Â mÃ´ táº£ xÃ¡c suáº¥t cá»§a tá»«ng káº¿t quáº£Â $(s', r)$.

**CÃ´ng thá»©c:**

$$
P(s', r | s, a) = â„™(S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a)
$$

$p(s',r|s,a)$ lÃ  xÃ¡c suáº¥t xáº£y ra tráº¡ng thÃ¡i $s'$ vÃ  nháº­n Ä‘Æ°á»£c pháº§n thÆ°á»Ÿng $r$ vá»›i hÃ nh Ä‘á»™ng $a$ tá»« tráº¡ng thÃ¡i $s$

**Giáº£i thÃ­ch:** 

- HÃ m nÃ y Ä‘á»‹nh nghÄ©a "luáº­t chÆ¡i" cá»§a mÃ´i trÆ°á»ng:
  
  - **Äáº§u vÃ o:**Â Tráº¡ng thÃ¡i hiá»‡n táº¡iÂ `s`Â + hÃ nh Ä‘á»™ngÂ `a`.
  - **Äáº§u ra:**Â PhÃ¢n phá»‘i xÃ¡c suáº¥t cho táº¥t cáº£ cáº·pÂ `(s', r)`Â cÃ³ thá»ƒ.

- VÃ­ dá»¥: Robot di chuyá»ƒn nhÆ°ng cÃ³ 10% kháº£ nÄƒng trÆ°á»£t (sang tráº¡ng thÃ¡i khÃ¡c ngoÃ i dá»± Ä‘á»‹nh).

**TÃ­nh cháº¥t:**

* âˆ‘â‚›',áµ£ P(s', r | s, a) = 1 (tá»•ng xÃ¡c suáº¥t báº±ng 1)
  
  $$
  \begin{gather*}
p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1] \\
\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) = 1, \forall s \in \mathcal{S},\ a \in \mathcal{A}(s)
\end{gather*}
  $$

* MÃ´i trÆ°á»ngÂ **deterministic**Â lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t: $P(s', r | s, a) = 1$ cho má»™t cáº·pÂ $(s', r)$Â duy nháº¥t.

* TÃ­nh cháº¥t Markov: tráº¡ng thÃ¡i tÆ°Æ¡ng lai chá»‰ phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i vÃ  hÃ nh Ä‘á»™ng hiá»‡n táº¡i, khÃ´ng pháº£i vÃ o chuá»—i cÃ¡c sá»± kiá»‡n trÆ°á»›c nÃ³ (ghi nhá»› vá» cÃ¡c tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³ khÃ´ng giÃºp cáº£i thiá»‡n cho dá»± Ä‘oÃ¡n vá» tÆ°Æ¡ng lai)

#### Video: Examples of MDPs

**Dynamics of the Recycling Robot**

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/8897e08c-f373-457e-8eb2-42848d7d5536.png" alt="8897e08c-f373-457e-8eb2-42848d7d5536" data-align="center" style="zoom:33%;">

state, action cÃ³ thá»ƒ Ä‘Æ¡n giáº£n, chi tiáº¿t (low-level) hoáº·c abstract (high-level)

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/d792b477-04bc-403d-84aa-779489a71c20.png" alt="d792b477-04bc-403d-84aa-779489a71c20" style="zoom:67%;" data-align="center">

### Goal of Reinforcement Learning

#### Video: The Goal of Reinforcement Learning

##### Goal of an Agent: Formal definition

Má»¥c tiÃªu cá»§a agent lÃ  tá»‘i Ä‘a hÃ³a tá»•ng pháº§n thÆ°á»Ÿng nháº­n vá» (return a.k.a giÃ¡ trá»‹ tÃ­ch lÅ©y) trong tÆ°Æ¡ng lai

$$
G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + ...
$$

$G_t$ lÃ  1 biáº¿n ngáº«u nhiÃªn

$$
\begin{gather*}
\mathbb{E}[G_t] = \mathbb{E}[\ R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T \ ]

\\\\
\textbf{\textcolor{red}{maximize the expected return}}
\end{gather*}
$$

Tá»‘i Ä‘a hÃ³a giÃ¡ trá»‹ mong Ä‘á»£i cho pháº§n thÆ°á»Ÿng nháº­n Ä‘Æ°á»£c trong tÆ°Æ¡ng lai (tá»•ng cÃ¡c pháº§n thÆ°á»Ÿng pháº£i lÃ  1 sá»‘ há»¯u háº¡n)

$T$ lÃ  thá»i Ä‘iá»ƒm káº¿t thÃºc (final timestep)

##### Epsodic Tasks (táº­p hÃ nh Ä‘á»™ng)

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/b77e2e6c-1f87-478c-bf13-236ad4ebfed4.png" alt="b77e2e6c-1f87-478c-bf13-236ad4ebfed4" data-align="center" style="zoom:67%;">

CÃ¡c tÆ°Æ¡ng tÃ¡c Ä‘Æ°á»£c chia thÃ nh cÃ¡c Ä‘oáº¡n nhá» gá»i lÃ  cÃ¡c episode,  má»—i episode báº¯t Ä‘áº§u Ä‘á»™c láº­p vá»›i káº¿t thÃºc cá»§a episode trÆ°á»›c Ä‘Ã³, khi 1 episode káº¿t thÃºc, agent Ä‘Æ°á»£c Ä‘áº·t láº¡i vá» tráº¡ng thÃ¡i báº¯t Ä‘áº§u, má»—i episode sáº½ cÃ³ 1 tráº¡ng thÃ¡i káº¿t thÃºc.

VÃ­ dá»¥: 

* ChÆ¡i má»™t vÃ¡n cá» (má»—i vÃ¡n lÃ  má»™t episode) -> tráº¡ng thÃ¡i káº¿t thÃºc: tháº¯ng, hÃ²a, Ä‘áº§u hÃ ng

* Má»™t láº§n robot di chuyá»ƒn tá»« vá»‹ trÃ­ xuáº¥t phÃ¡t Ä‘áº¿n Ä‘Ã­ch

##### Discussion Prompt: Is the reward hypothesis sufficient?

- Multi-objective tasks:Â Canâ€™t balance goals like speed and safety in self-driving cars.

- Risk-sensitive tasks:Â Ignores risk, e.g., investors want to avoid rare catastrophic losses, not just maximize average return.

- Modal/counterfactual tasks:Â Canâ€™t reason about what could have happened, e.g., a robot must maintain the ability to avoid collisions.

- Dynamic/subjective preferences:Â Canâ€™t adapt to changing or conflicting values, e.g., recommenders may need to shift from engagement to well-being.

- Non-Markovian dependencies:Â Canâ€™t encode history-based requirements, e.g., loan systems must ensure fairness over time.

#### Video: Michael Littman: The Reward Hypothesis

##### Whence Behavior?

- give a man a fish and he'll eat for a day
  
  - Programming (GOFAI -  Good-old fashioned AI)

- teach a man to fish and he'll eat for a lifetime
  
  - Examples (LfD)

- give a man a taste for fish and he'll figure out how to get fish, even if the details change!
  
  - Optimization (RL)

##### Goals as Rewards

- 1 for goal, 0 otherwise
  
  - goal-reward representation

- -1 for not goal, 0 once goal reached
  
  - action-penalty representation

##### Whence Rewards?

- Programming 
  
  - Coding
  
  - Human-in-the-loop (con ngÆ°á»i hay thay Ä‘á»•i cÃ¢u tráº£ lá»i cá»§a mÃ¬nh tÃ¹y theo cÃ¡ch mÃ  agent Ä‘ang há»c - non-stationary reward)

- Examples
  
  - Mimic reward (Ä‘Æ°a ra vÃ­ dá»¥ vá» pháº§n thÆ°á»Ÿng bá»Ÿi vÃ­ dá»¥, agent sáº½ há»c cÃ¡ch "sao chÃ©p" pháº§n thÆ°á»Ÿng Ä‘Æ°á»£c cung cáº¥p, reward -> behavior)
  
  - Inverse reinforcement learning (trainer cho agent há»c báº±ng viá»‡c cung cáº¥p vÃ­ dá»¥ vá» "hÃ nh vi" mong muá»‘n Ä‘á»ƒ agent tá»± suy luáº­n ra reward má»¥c tiÃªu cáº§n Ä‘áº¡t Ä‘Æ°á»£c tá»‘i Æ°u tÆ°Æ¡ng Ä‘Æ°Æ¡ng cho "hÃ nh vi" Ä‘Æ°á»£c cung cáº¥p, behavior -> reward)

- Optimization
  
  - Evolutionary optimization: cho nhiá»u model cÃ¹ng há»c vÃ  model nÃ o "sá»‘ng sÃ³t" (cÃ³ káº¿t quáº£ vÃ  thuáº­t toÃ¡n tá»‘t hÆ¡n 1 ngÆ°á»¡ng) thÃ¬ sáº½ Ä‘Æ°á»£c giá»¯ láº¡i cho cÃ¡c bÆ°á»›c há»c tiáº¿p theo
  
  - Meta RL

##### Challenges to the Hypothesis (Nhá»¯ng thÃ¡ch thá»©c cá»§a giáº£ thuyáº¿t pháº§n thÆ°á»Ÿng)

- Má»¥c tiÃªu khÃ´ng pháº£i lÃ  giÃ¡ trá»‹ ká»³ vá»ng cá»§a pháº§n thÆ°á»Ÿng tÃ­ch lÅ©y 
  
  - Risk-averse problem (giáº£m thiá»ƒu rá»§i ro): chá»n káº¿t quáº£ tá»‘t nháº¥t mÃ  Ã­t kháº£ nÄƒng cÃ³ biáº¿n cá»‘ xáº£y ra nháº¥t, cÃ³ thá»ƒ mÃ´ hÃ¬nh hÃ³a báº±ng viá»‡c khuáº¿ch Ä‘áº¡i cÃ¡c káº¿t quáº£ tiÃªu cá»±c trong pháº§n thÆ°á»Ÿng
  
  - CÃ¢n báº±ng Ä‘a dáº¡ng hÃ³a

- CÃ³ thá»ƒ tÆ°Æ¡ng thÃ­ch vá»›i cÃ¡ch há»c nhÆ° cá»§a con ngÆ°á»i khÃ´ng?
  
  - Theo Ä‘uá»•i 1 nhiá»‡m vá»¥ mÃ¹ quÃ¡ng khÃ´ng pháº£i luÃ´n luÃ´n tá»‘t
  
  - Má»¥c tiÃªu cÃ³ thá»ƒ thay Ä‘á»•i, phÃ¡t triá»ƒn theo thá»i gian

### Continuing Tasks

#### Video: Continuing Tasks

| Episodic Tasks                                         | Continuing Tasks                                 |
| ------------------------------------------------------ | ------------------------------------------------ |
| Interaction breaks naturally into episodes             | Interaction goes on continually                  |
| Each episode ends in a terminal state                  | No terminal state                                |
| Episodes are independent                               | Results are dependent                            |
| $G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T$ | $G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + \dots$ |

##### Example: Smart thermostat which regulates the temperature of a building

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/6958a5bb-ad70-4072-83bf-e07cc16209c3.png" alt="6958a5bb-ad70-4072-83bf-e07cc16209c3" data-align="center" style="zoom:33%;">

##### Discounting

$$
\begin{align*}
G_t &\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{k-1} R_{t+k} + \dots  
\\
&= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{align*}
$$

$G_t$ luÃ´n lÃ  há»¯u háº¡n khi $0 \leq \gamma < 1$, $\gamma$ lÃ  tá»· sá»‘ chiáº¿t kháº¥u

###### Chá»©ng minh ráº±ng $G_t$ há»¯u háº¡n khi $0 \leq \gamma < 1$

Giáº£ sá»­Â $R_{max}$Â lÃ  pháº§n thÆ°á»Ÿng tá»‘i Ä‘a mÃ  agent  cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c

$$
\begin{align*}
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
   \leq \sum_{k=0}^{\infty} \gamma^k R_{\max}
   &= R_{\max} \sum_{k=0}^{\infty} \gamma^k \\
   &= R_{\max} \times \frac{1}{1 - \gamma}
\end{align*}


$$

MÃ  $\sum_{k=0}^{\infty} \gamma^k$ lÃ  tá»•ng cá»§a 1 chuá»—i hÃ¬nh há»c há»™i tá»¥ (chuá»—i cÃ³ tá»•ng tiáº¿n tá»›i má»™t giÃ¡ trá»‹ há»¯u háº¡n) khi $\gamma < 1$ cÃ³ tá»•ng há»™i tá»¥ vá»Â $\frac{1}{1 - \gamma}$.

$\Rightarrow R_{\max} \times \frac{1}{1 - \gamma}$ lÃ  há»¯u háº¡n vÃ  lÃ  cáº­n trÃªn cá»§a $G_t$ 

$\Rightarrow G_t$ lÃ  há»¯u háº¡n

##### Effect of $\gamma$ on agent behavior

- $\gamma = 0$
  
  $$
  \begin{align*}
G_t &\doteq R_{t+1} + \textcolor{red}\gamma R_{t+2} + \textcolor{red}{\gamma^2} R_{t+3} + \dots + \textcolor{red}{\gamma^{k-1}} R_{t+k} + \dots \\
&= R_{t+1} + \textcolor{red}{0} R_{t+2} + \textcolor{red}{0^2} R_{t+3} + \dots + \textcolor{red}{0^{k-1}} R_{t+k} + \dots \\
&= R_{t+1}
\end{align*}
  $$
  
  Khi Â $\gamma = 0$, giÃ¡ trá»‹ tÃ­ch lÅ©y $G_t$ sáº½ chá»‰ Ä‘Æ¡n giáº£n lÃ  reward á»Ÿ bÆ°á»›c tiáº¿p theo, agent sáº½ cÃ³ táº§m nhÃ¬n ngáº¯n háº¡n vÃ  chá»‰ quan tÃ¢m Ä‘áº¿n pháº§n thÆ°á»Ÿng mong Ä‘á»£i tá»©c thá»i 

- $\gamma \to 1 $
  
  $$
  \begin{align*}
G_t &\doteq R_{t+1} + \textcolor{red}\gamma R_{t+2} + \textcolor{red}{\gamma^2} R_{t+3} + \dots + \textcolor{red}{\gamma^{k-1}} R_{t+k} + \dots \\
&\approx R_{t+1} + \textcolor{red}{1} R_{t+2} + \textcolor{red}{1^2} R_{t+3} + \dots + \textcolor{red}{1^{k-1}} R_{t+k} + \dots \\
&= R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_{t+k} + \dots
\end{align*}
  $$
  
  Pháº§n thÆ°á»Ÿng tá»©c thá»i vÃ  trong tÆ°Æ¡ng lai cÃ³ Ä‘á»™ quan trá»ng gáº§n báº±ng nhau nÃªn agent sáº½ cÃ³ táº§m nhÃ¬n xa hÆ¡n 

##### Recursive nature of returns (tÃ­nh Ä‘á»‡ quy cá»§a giÃ¡ trá»‹ tÃ­ch lÅ©y)

$$
\begin{align*}
G_t &\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots \\
&= R_{t+1} + \gamma \left( R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots \right) \\
G_t &= R_{t+1} + \gamma G_{t+1}
\end{align*}
$$

## Module 4

### Policies and Value Functions

#### Video: Specifying Policies

##### Deterministic Policy (chÃ­nh sÃ¡chÂ xÃ¡c Ä‘á»‹nh)

$\pi$ (policy: chÃ­nh sÃ¡ch) lÃ  phÆ°Æ¡ng phÃ¡p lá»±a chá»n (Ã¡nh xáº¡) hÃ nh Ä‘á»™ng $a$ tá»« tráº¡ng thÃ¡i $s$.<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/f4b45efa-e490-418c-b1ad-25f60e0dafee.png" alt="f4b45efa-e490-418c-b1ad-25f60e0dafee" style="zoom:50%;" data-align="center">

##### Stochastic Policy (chÃ­nh sÃ¡chÂ ngáº«u nhiÃªn)

$\pi(a|s)$ lÃ  xÃ¡c suáº¥t lá»±a chá»n hÃ nh Ä‘á»™ng $a$ khi Ä‘ang á»Ÿ tráº¡ng thÃ¡i $s$, tá»•ng xÃ¡c suáº¥t cá»§a táº¥t cáº£ hÃ nh Ä‘á»™ng $a$ trong tráº¡ng thÃ¡i $s$ pháº£i báº±ng 1 vÃ  khÃ´ng cÃ³ xÃ¡c suáº¥t hÃ nh Ä‘á»™ng nÃ o Ä‘Æ°á»£c lÃ  sá»‘ Ã¢m.

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/2e5e7d2e-9819-4919-8b95-43ba543e7f5f.png" alt="2e5e7d2e-9819-4919-8b95-43ba543e7f5f" style="zoom:33%;" data-align="center">

###### Stochastic Policy Example

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/1f2e3f92-db5d-40a3-843f-d83d2a35d86b.png" alt="1f2e3f92-db5d-40a3-843f-d83d2a35d86b" style="zoom:33%;" data-align="center">

##### Valid and invalid policies

Policy trong MDPs chá»‰ quyáº¿t Ä‘á»‹nh hÃ nh Ä‘á»™ng tiáº¿p theo dá»±a trÃªn tráº¡ng thÃ¡i $s$ hiá»‡n táº¡i, khÃ´ng phá»¥ thuá»™c vÃ o cÃ¡c hÃ nh Ä‘á»™ng hay tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³. Trong MDPs, tráº¡ng thÃ¡i hiá»‡n táº¡i luÃ´n chá»©a Ä‘áº§y Ä‘á»§ thÃ´ng tin cho viá»‡c ra quyáº¿t Ä‘á»‹nh cho hÃ nh Ä‘á»™ng tiáº¿p theo.

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/34469551-a1f8-4cfa-9f07-8da576c60176.png" alt="34469551-a1f8-4cfa-9f07-8da576c60176" style="zoom:33%;" data-align="center">

#### Video: Value Functions

##### State-value functions

GiÃ¡ trá»‹ tÃ­ch lÅ©y ká»³ vá»ng khi báº¯t Ä‘áº§u á»Ÿ tráº¡ng thÃ¡i $s$ vá»›i chÃ­nh sÃ¡ch $\pi$ 

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/04183a6a-c14a-4fd5-a7f7-c34e7f583bc6.png" alt="04183a6a-c14a-4fd5-a7f7-c34e7f583bc6" style="zoom:50%;" data-align="center">

##### Action-value functions

GiÃ¡ trá»‹ tÃ­ch lÅ©y ká»³ vá»ng khi báº¯t Ä‘áº§u á»Ÿ tráº¡ng thÃ¡i $s$, thá»±c hiá»‡n hÃ nh Ä‘á»™ng $a$ vá»›i chÃ­nh sÃ¡ch $\pi$

$$
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi} \left[ G_t \mid S_t = s, A_t = a \right]
$$

Value functions represent how good it is for the agent to be in a given state or take a particular action in a given state under a particular policy.

Value functions are crucial for reinforced learning because they allow the agent to query the quality of the current situations instead of waiting to observe the long-term outcome:

- The return is not immediately available

- The return can be random due to stochasticity in both the policy and environment dynamics

#### Video: Bellman Equation

##### State-value Bellman equation

$$
\begin{align*}
v_\pi(s) &\doteq \mathbb{E}_\pi[G_t \mid S_t = s] \\
         &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
         &= \sum_a \pi(a|s) \sum_{s'} \sum_r p(s', r \mid s, a) \left[ r + \gamma \mathbb{E}[G_{t+1} \mid S_{t+1} = s'] \right] \\
         &= \sum_a \pi(a|s) \sum_{s'} \sum_r p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right]
\end{align*}
$$

##### Action-value Bellman equation

$$
\begin{align*}
q_\pi(s, a) &\doteq \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \\
&= \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s'] \right] \\
&= \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') \mathbb{E}_\pi[G_{t+1} \mid S_{t+1} = s', A_{t+1} = a'] \right] \\
&= \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_\pi(s', a') \right]
\end{align*}
$$

### Optimality (Optimal Policies & Value Functions)

#### Video: Optimal Policies

The role of policies in Reinforcement Learning (RL) is pivotal as they dictate the behavior of an agent within its environment.

- Action Selection: Policies determine how an agent selects actions in different states of the environment. 

- Learning Objective: Policies define the learning objective for the RL agent.

- Exploration vs. Exploitation: Policies balance exploration and exploitation

##### Bellman Optimality Equation

An optimal policy $\pi_âˆ—$ is a policy that maximizes the expected cumulative reward over time. 

The Bellman optimality equation expresses the optimal value of a state (or state-action pair) in terms of the maximum expected immediate reward plus the discounted value of the successor states.

###### State-value Bellman optimality function $v_*(s)$

$$
v_*(s) = \max_a \sum_{s'} \sum_r p(s', r \mid s, a) \left[ r + \gamma v_*(s') \right]
$$

$v_*(s)$ represents the optimal value of state $s$ under the optimal policy

###### Action-value Bellman optimality function $q_*(s)$

$$
q_*(s, a) = \sum_{s'} \sum_{r} p(s', r \mid s, a) \left[ r + \gamma \max_{a'} q_*(s', a') \right]
$$

$q_*(s, a)$ represents the optimal value of taking action $a$ in state $s$ under the optimal policy.

##### Relationship

$v_*$ is equal to the maximum of the boxed term over all actions. $\pi_*$ is the argmax, which simply means the particular action which achieves this maximum.

$$
\pi_*(s) = \underset{a}{\operatorname{argmax}} \sum_{s'} \sum_r p(s',r|s,a)[r + \gamma v_*(s')]
$$

## Module 5

### Policy Evaluation (Prediction)

#### Video: Policy Evaluation vs. Control

![802a1bb6-a96a-4cdf-8695-6dd34b87cdfe](file:///D:/Disk/Computer/Pictures/Typedown/802a1bb6-a96a-4cdf-8695-6dd34b87cdfe.png)

* **Policy Evaluation**: Determines the value function of a given policy, assessing how good it is.

* **Policy Control**: Improves the policy to maximize future rewards, finding the optimal strategy.

* **Relative Value**: A policy is considered as good as or better than a policy if the value function under is greater than or equal to the value function under for every state. If there exists at least one state where has a strictly higher value, then is strictly better than .

#### Video: Iterative Policy Evaluation

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/6f612517-4a8f-4112-b8d4-7ba29a6468cf.png" alt="6f612517-4a8f-4112-b8d4-7ba29a6468cf" style="zoom:33%;" data-align="center">

The **dynamics of the environment** in reinforcement learning refer to how the agent interacts with the environment and how the environment responds. This is typically modeled as a **Markov Decision Process (MDP)** with the following components:

1. **States ($S$)**: The possible situations the agent can be in.

2. **Actions ($A$)**: The choices available to the agent in each state.

3. **Transition Probabilities ($ğ‘ƒ(ğ‘ â€²âˆ£ğ‘ ,ğ‘)$)**: The probability of moving to a new state given the current state and action .

4. **Reward Function ($R(s,a,s')$)**: The numerical feedback given to the agent when transitioning between states due to an action.

5. **Policy ($\pi(ğ‘âˆ£ğ‘ )$)**: The strategy the agent follows when choosing actions.

### Policy Iteration (Control)

We can find the optimal policy by choosing the Greedy action. The Greedy action maximizes the Bellman's optimality equation in each state. 

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/07025877-8be9-4474-b900-90bdaf27b332.png" alt="07025877-8be9-4474-b900-90bdaf27b332" style="zoom:50%;" data-align="center">

$$
\begin{aligned}
    q_{\pi'}(s, \pi'(s)) &\geq q_{\pi}(s, \pi(s)) && \text{for all } s \in \mathcal{S}
    &\boldsymbol{\rightarrow} \pi' \geq \pi  \\

    q_{\pi'}(s, \pi'(s)) &> q_{\pi}(s, \pi(s)) && \text{for at least one } s \in \mathcal{S} 
    &\boldsymbol{\rightarrow} \pi' > \pi
\end{aligned}

$$

Suppose you have two policies and their value functions for all states:

* IfÂ $v_{Ï€_2}(s)â‰¥v_{Ï€_1}(s)$Â for everyÂ $s$,Â $Ï€_2$Â is at least as good asÂ $Ï€_1$.

* If for some stateÂ $sâˆ—$,Â $v_{Ï€_2}(s*)>v_{Ï€_1}(s*)$, thenÂ $Ï€_2$Â is **strictly** better.

##### Policy Iteration

Policy iteration consists of two distinct steps: evaluation and improvement. 

1. **Evaluation Step**: We first evaluate our current policy $\pi_1$, which gives us a new value function $v_{\pi_1}$ that accurately reflects the value of $\pi_1$. 

2. **Improvement Step**: The improvement step then uses $v_{\pi_1}$ to produce a greedy policy $\pi_2$. At this point, $\pi_2$ is greedy with respect to the value function of $\pi_1$, but $v_{\pi_1}$ no longer accurately reflects the value of $\pi_2$, so we need to re-calculate again.
   
   

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/0c6ebbb3-7c3a-49fc-be37-5d1e54ccf497.png" alt="0c6ebbb3-7c3a-49fc-be37-5d1e54ccf497" data-align="center" style="zoom:67%;">

###### Policy Iteration pseudo code using iterative policy evaluation for estimate pi ~ pi*

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/dac15803-10d2-4744-bfb2-f0086a74d759.png" alt="dac15803-10d2-4744-bfb2-f0086a74d759" data-align="center" style="zoom:50%;">

### Generalized Policy Iteration

#### Value Iteration

Value Iteration for estimating p~p*

<img title="" src="file:///D:/Disk/Computer/Pictures/Typedown/2c77e425-8795-4a5e-8e69-812065a7fb0d.png" alt="2c77e425-8795-4a5e-8e69-812065a7fb0d" style="zoom:50%;" data-align="center">


